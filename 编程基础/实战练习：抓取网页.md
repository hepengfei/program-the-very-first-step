

学习了前面的基础内容，我就今天就来进行实战练习，写一个最简单的爬虫：抓取一个网页。

在我们没有任何网络基础知识的情况下，如何抓取网页呢？答案是：站在专家的肩膀上。

各个编程语言中都有丰富的库函数，这些库函数都是由相关领域的专家写出来的。正是由于函数的可重用性这一特性，才使得大家能简单的重复使用专家的工作成果。

每个库通常都有完善的文档说明（如果没有文档，就将代码看作文档，直接看代码去）。通过阅读文档，能快速掌握该库的功能、作用和适用场景等等。

要抓取网页，我们需要使用第三方库`urllib`。我先给出示例代码。

在python2.x中，使用`urllib2`。

```py
# 抓取1个网页 - python 2.x
import urllib2
url = 'http://docs.python.org/3/tutorial/controlflow.html'
page = urllib2.urlopen(url)
content = page.read()
print(content)

path = "/tmp/controlflow.html"
f = open(path, "w+")
f.write(content)
f.close()
```

在python3.x中，它分为了`urllib.request`和`urllib.response`等子库。

```py
# 抓取1个网页 - python 3.x
import urllib.request
url = 'http://docs.python.org/3/tutorial/controlflow.html'
page = urllib.request.urlopen(url)
content = page.read()
print(content)

path = "/tmp/controlflow.html"
f = open(path, "w+")
f.write(str(content))
f.close()
```

可以看出，上面的主要步骤是：

1. 使用`urlopen`打开一个网页地址（即url），得到一个网页数据`page`
2. 从网页数据中读取内容，即抓取到的网页源代码
3. 创建一个文件
4. 将网页源代码写入该文件

然后，大家可以分别使用浏览器和记事本打开该文件对比查看一下，理解一下浏览器显示的网页内容和网页源代码的区别和关系。

大家自己重新写一下自己的抓取代码，试试抓取一个自己想抓的页面。


要懂网页抓取，会写较复杂的爬虫，一定要懂以下基本内容：

1. HTML
2. HTTP协议
3. 正则表达式


我希望通过本文中，大家能掌握阅读文档的能力。掌握这一能力，就能自主的吸取新的知识，加快学习的速度。对于程序员，阅读文档是一项非常基础和非常关键的能力。

计算机相关的文档，一定要读官方英文文档。你可以读其它资料，但官方英文文档一定要通读。很多时候，翻译文档的人并不是程序员，词不达意的情况经常能遇到。

不要害怕自己的英文能力不行。编程语言等专业性的文档，词汇量极少，句式也非常固定。大家只需要能坚持读，很快就会非常熟练。

### 练习
抓取回来的网页数据`page`，除了网页源代码，还有哪些内容？请查阅相关文档，将其它内容也打印出来。

### python参考链接
在本文的代码中，我们使用了内置函数`open`、内置对象（也可以理解为内置库）`file`、第三方库`urllib`，请查阅以下链接，以深入了解这些内容。

- 内置函数 open https://docs.python.org/2/library/functions.html#open
- 内置对象 file https://docs.python.org/2/library/stdtypes.html#file-objects
- 第三方库-python2.x urllib2  https://docs.python.org/2/library/urllib2.html
- 第三方库-python3.x urllib.request https://docs.python.org/3/library/urllib.request.html#module-urllib.request
- 第三方库-python3.x urllib.response https://docs.python.org/3/library/urllib.request.html#module-urllib.response


### 答疑

#### HTTPS
有的同学使用过程中，发碰到以下警告输出：

```
/Library/Python/2.7/site-packages/requests/packages/urllib3/connectionpool.py:789: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
```

阅读警告内容，可以发现它是由于使用了HTTPS链接引起的。因此可以想到换成http链接就可以消除了。

如果必须要用https链接，我们进一步看警告输出的链接内容。其中，有明确的关于`InsecureRequestWarning`的解释：

> This happens when an request is made to an HTTPS URL without certificate verification enabled. Follow the certificate verification guide to resolve this warning.

跟着网页中的链接[certificate verification guide](https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl)可以找到解决方案，也就是增加HTTPS认证支持。按照文档中介绍的操作即可。

警告输出的链接内容的后面，还给出了关闭输出警告的接口和例子。所以也可以简单的将警告关掉，来消除它。在抓取网页的例子中，我们不太关心HTTPS认证，可以采用这种方式。

总结下来，有3种方式消除：
1. 将https://换成http://（文章中的链接可以替换替换）
2. 禁止输出警告 urllib3.disable_warnings() 
3. 按指引 [certificate verification guide](https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl) 中操作

可以看到，通过仔细阅读和查阅文档，我们总是可以解决碰到的问题的。


