

学习了前面的基础内容，我就今天就来进行实战练习，写一个最简单的爬虫：抓取一个网页。

在我们没有任何网络基础知识的情况下，如何抓取网页呢？答案是：站在专家的肩膀上。

各个编程语言中都有丰富的库函数，这些库函数都是由相关领域的专家写出来的。正是由于函数的可重用性这一特性，才使得大家能简单的重复使用专家的工作成果。

每个库通常都有完善的文档说明（如果没有文档，就将代码看作文档，直接看代码去）。通过阅读文档，能快速掌握该库的功能、作用和适用场景等等。

要抓取网页，我们需要使用第三方库`urllib`。我先给出示例代码。

在python2.x中，使用`urllib2`。

```py
# 抓取1个网页 - python 2.x
import urllib2
url = 'https://docs.python.org/3/tutorial/controlflow.html'
page = urllib2.urlopen(url)
content = page.read()
print(content)

path = "/tmp/controlflow.html"
f = open(path, "w+")
f.write(content)
f.close()
```

在python3.x中，它分为了`urllib.request`和`urllib.response`等子库。

```py
# 抓取1个网页 - python 3.x
import urllib.request
url = 'https://docs.python.org/3/tutorial/controlflow.html'
page = urllib.request.urlopen(url)
content = page.read()
print(content)

path = "/tmp/controlflow.html"
f = open(path, "w+")
f.write(str(content))
f.close()
```

可以看出，上面的主要步骤是：

1. 使用`urlopen`打开一个网页地址（即url），得到一个网页数据`page`
2. 从网页数据中读取内容，即抓取到的网页源代码
3. 创建一个文件
4. 将网页源代码写入该文件

然后，大家可以分别使用浏览器和记事本打开该文件对比查看一下，理解一下浏览器显示的网页内容和网页源代码的区别和关系。

要懂网页抓取，会写较复杂的爬虫，一定要懂以下基本内容：

1. HTML
2. HTTP协议
3. 正则表达式


我希望通过本文中，大家能掌握阅读文档的能力。掌握这一能力，就能自主的吸取新的知识，加快学习的速度。对于程序员，阅读文档是一项非常基础和非常关键的能力。

计算机相关的文档，一定要读官方英文文档。你可以读其它资料，但官方英文文档一定要通读。很多时候，翻译文档的人并不是程序员，词不达意的情况经常能遇到。

不要害怕自己的英文能力不行。编程语言等专业性的文档，词汇量极少，句式也非常固定。大家只需要能坚持读，很快就会非常熟练。

### 练习
抓取回来的网页数据`page`，除了网页源代码，还有哪些内容？请查阅相关文档，将其它内容也打印出来。

### python参考链接
在本文的代码中，我们使用了内置函数`open`、内置对象（也可以理解为内置库）`file`、第三方库`urllib`，请查阅以下链接，以深入了解这些内容。

- 内置函数 open https://docs.python.org/2/library/functions.html#open
- 内置对象 file https://docs.python.org/2/library/stdtypes.html#file-objects
- 第三方库-python2.x urllib2  https://docs.python.org/2/library/urllib2.html
- 第三方库-python3.x urllib.request https://docs.python.org/3/library/urllib.request.html#module-urllib.request
- 第三方库-python3.x urllib.response https://docs.python.org/3/library/urllib.request.html#module-urllib.response
